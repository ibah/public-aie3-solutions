{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "âœ‹BREAKOUT ROOM #1:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: Basic RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "\n",
        "âœ‹BREAKOUT ROOM #2:\n",
        "- Task 1: Parent Document Retriever\n",
        "- Task 2: Ensemble Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "9cc0c072-1117-4863-8010-ee37e8e33a3d"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in c:\\users\\user\\projects\\aie3\\public-aie3-solutions\\.venv\\lib\\site-packages (5.2.2)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting beautifulsoup4 (from bs4)\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
            "   --------------------------- ------------ 102.4/147.9 kB 3.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 147.9/147.9 kB 1.8 MB/s eta 0:00:00\n",
            "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
            "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "a5794372-be42-46ee-cf7d-4e5628e97e9a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NpPwk1YAgl"
      },
      "source": [
        "## Task 2: Basic RAG Chain\n",
        "\n",
        "Now we'll set up our basic RAG chain, first up we need a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUWXhsNVYLTA"
      },
      "source": [
        "### OpenAI Model\n",
        "\n",
        "\n",
        "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
        "\n",
        "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CSgK6jgw_tI3"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiagvgVDYTPn"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ntIqnv4cA5gR"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDO0XJqbYabb"
      },
      "source": [
        "### SiteMap Loader\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAS3QBQSARiw",
        "outputId": "c10b7cc4-e5c7-4dd8-c689-59d5c356c4d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching pages: 100%|##########| 219/219 [00:14<00:00, 15.60it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_s_x87H0BYmn",
        "outputId": "2c74e338-8ba6-432f-c2da-641d5d55336e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://blog.langchain.dev/langgraph-cloud/'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F79PdFcaYfBL"
      },
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We're going to use a relatively naive text splitting strategy today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NmCdYTTTA4du"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLA5-LNBBVM-",
        "outputId": "f5a05c71-c382-42f4-e6a5-58f3b61d66f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUsEc07iYnwj"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging OpenAI's [text-embedding-3-small](https://openai.com/index/new-embedding-models-and-api-updates/) today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QVhMN0aaBrsM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoO_2MaY0TS"
      },
      "source": [
        "### Qdrant VectorStore Retriever\n",
        "\n",
        "Now we can use a Qdrant VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nBTK9kSFBWM1"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    split_documents,\n",
        "    base_embeddings_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"langchainblogs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZpwDxlniCJRu"
      },
      "outputs": [],
      "source": [
        "base_retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GPhHPAY5yG"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "All we have left is a prompt template, which we'll create here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YAU74penCNmR"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmT5VyLmZAAK"
      },
      "source": [
        "### LCEL Chain\n",
        "\n",
        "Now that we have:\n",
        "\n",
        "- Embeddings Model\n",
        "- Generation Model\n",
        "- Retriever\n",
        "- Prompt\n",
        "\n",
        "We're ready to build our LCEL chain!\n",
        "\n",
        "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pqVAsUc_Cp-7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "base_rag_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNjMoS-ZVo5"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6Dq9rCScDfBE",
        "outputId": "4bcebeb0-37ae-4fb8-9dae-ceba9cc1dc54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A good way to evaluate agents is by testing their capabilities in tasks that are prerequisites for common agentic workflows, such as planning, task decomposition, function calling, and the ability to override pre-trained biases when needed. Additionally, evaluating the behavior patterns of the agents in the specific tasks they are expected to excel in before deployment is crucial for validation.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "587380f0-7395-4608-aa63-35d117dbd162"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3eoqBtBQERXP",
        "outputId": "727abc25-3510-49b7-9671-98406e672294"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM applications developed by LangChain. It provides visual feedback on outputs, accuracy metrics, evaluation of performance nuances, and monitoring capabilities for AI applications. LangSmith is designed to complement LangChain and assist users in managing their LLM applications effectively.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMzWpDK369i2"
      },
      "source": [
        "![trace_screenshots](./assets/trace_screenshot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Create Testing Dataset\n",
        "\n",
        "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
        "\n",
        "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCKCAhASkXu0"
      },
      "source": [
        "### Synthetic Data Generation (SDG)\n",
        "\n",
        "In order to full test our RAG chain, and the various modifications we'll be using in the following notebook, we'll need to create a small synthetic dataset that is relevant to our task!\n",
        "\n",
        "Let's start by generating a series of questions - which begins with a simple model definition!\n",
        "\n",
        "> NOTE: We're going to be using a purposefully simplified datagen pipeline as an example today - but you could leverage the RAGAS SDG pipeline just as easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-dgAkRzDlEgH"
      },
      "outputs": [],
      "source": [
        "question_model = ChatOpenAI(model=\"gpt-3.5-turbo\") # ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-HC4C8lWIS"
      },
      "source": [
        "Next up, we'll create some novel chunks from our source data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "2Sz7rw0-lhf8"
      },
      "outputs": [],
      "source": [
        "sdg_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pep-eqUbllrv"
      },
      "source": [
        "Now, let's ask some questions that could be answered from the provided chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "e75gwn4Tlt3w"
      },
      "outputs": [],
      "source": [
        "question_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating questions for an exam. You must create a question for a given piece of context.\n",
        "\n",
        "The question must be answerable only using the provided context.\n",
        "\n",
        "Avoid creating questions that are ambiguous or vague. They should be specifically related to the context.\n",
        "\n",
        "Your output must only be the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "question_prompt = ChatPromptTemplate.from_template(question_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Pcb_R-G_odCl"
      },
      "outputs": [],
      "source": [
        "question_chain = question_prompt | question_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5YbGi7umtOB"
      },
      "source": [
        "Now we can loop through a subset of our context chunks and create question/context pairs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot0HzAGBms90",
        "outputId": "0417f0df-ea17-4da0-dd3b-a6336b154c4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/22 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:17<00:00,  1.29it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "question_context_pairs = []\n",
        "\n",
        "for idx in tqdm(range(0, len(sdg_documents), 40)):\n",
        "  question = question_chain.invoke({\"context\" : sdg_documents[idx].page_content})\n",
        "  question_context_pairs.append({\"question\" : question.content, \"context\" : sdg_documents[idx].page_content, \"idx\" : idx})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhg3AJlmfW-",
        "outputId": "9c32bad2-f2db-471d-e526-ad430e427292"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Question: What is the name of the new infrastructure for running agents at scale that is available in beta, as mentioned in the provided context?',\n",
              " 'context': 'Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\nOur new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. We also have a new stable release of LangGraph.\\n\\n6 min read\\nJun 27, 2024',\n",
              " 'idx': 0}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34zGDKtD9I0A"
      },
      "source": [
        "We'll repeat this process for answers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Ws624hGapJ92"
      },
      "outputs": [],
      "source": [
        "answer_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating an exam. You must create a answer for a given piece of context and question.\n",
        "\n",
        "The answer must only rely on the provided context.\n",
        "\n",
        "Your output must only be the answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_template(answer_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "AEnETuSEqf2R"
      },
      "outputs": [],
      "source": [
        "answer_chain = answer_prompt | question_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89faPBcPqnfT",
        "outputId": "8e9bd52a-b400-44da-d228-34d9234c324e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/22 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:16<00:00,  1.33it/s]\n"
          ]
        }
      ],
      "source": [
        "for question_context_pair in tqdm(question_context_pairs):\n",
        "  question_context_pair[\"answer\"] = answer_chain.invoke({\"question\" : question_context_pair[\"question\"], \"context\" : question_context_pair[\"context\"]}).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usB9PkJWq_0D",
        "outputId": "4cc1d45b-c389-4d18-a7cf-37e8efbdcf00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Question: What is the name of the new infrastructure for running agents at scale that is available in beta, as mentioned in the provided context?',\n",
              " 'context': 'Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\nOur new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. We also have a new stable release of LangGraph.\\n\\n6 min read\\nJun 27, 2024',\n",
              " 'idx': 0,\n",
              " 'answer': 'LangGraph v0.1 and LangGraph Cloud are now available in beta.'}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-v1\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in question_context_pairs:\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "65d1b263094a4c6abdaf8688285c41cc",
            "087e4c759a35424385156c76e3780144",
            "18ffc7ae91d1408c8747f8d0a972f3a8",
            "8cbac09f7ba540bd8f79dde47e49fe5f",
            "b5e2be01c1834c78b7ce0cbe1f7a9d09",
            "ef2e833ac73548ee837f906d4b004a3d",
            "2c984bd9861c4ec0b27f0c3a2020dede",
            "c9927b2052be4016b0feb88ab583d32e",
            "6fbddf6663954ff48ee0cb0b4a202acb",
            "e6035d68dd9e4c6598ac5eae2d0deac1",
            "b6b1f2be05a34e118dc55869149b60ac"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "528f629a-abf9-4f9b-9af4-3756aa40cc50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-5a6db824' at:\n",
            "https://smith.langchain.com/o/42414e9e-b418-58ec-bdcf-7afcba385c60/datasets/4584aa0d-24c9-40dc-8ad9-bc95d21c61ff/compare?selectedSessions=22f0fbc9-7080-4978-82d6-9d077d8d21e4\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 2da43ff8-dfed-4d9b-8d5e-ac6f96ce415f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9721, Requested 335. Please try again in 336ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9721, Requested 335. Please try again in 336ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2f1766bb-91ef-4978-b5b0-564f74ac293a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9948, Requested 275. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9948, Requested 275. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "1it [00:28, 28.74s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f7b8cc95-9ff7-4081-800f-037a78f124de: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9819, Requested 263. Please try again in 492ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9819, Requested 263. Please try again in 492ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "2it [00:30, 12.71s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 2fa0165d-b644-4597-9056-7f3b6b5cf6b4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9792, Requested 293. Please try again in 510ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9792, Requested 293. Please try again in 510ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "3it [00:30,  6.98s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6f202887-f540-47ae-ba03-98c58ce1ad35: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9816, Requested 359. Please try again in 1.05s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9816, Requested 359. Please try again in 1.05s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7d0a3bd2-0803-41fb-8223-cae4032df216: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9738, Requested 327. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9738, Requested 327. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "4it [00:31,  4.58s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run c99f9ace-93af-4138-bb2e-b7b64cd443ce: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9757, Requested 300. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9757, Requested 300. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "5it [00:31,  3.05s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 2da43ff8-dfed-4d9b-8d5e-ac6f96ce415f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9883, Requested 385. Please try again in 1.608s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9883, Requested 385. Please try again in 1.608s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "6it [00:32,  2.18s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6f202887-f540-47ae-ba03-98c58ce1ad35: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9842, Requested 380. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9842, Requested 380. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "7it [00:33,  1.92s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 406b6d2d-1b5e-4bf4-9f75-bb132608ca9d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9902, Requested 320. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9902, Requested 320. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "8it [00:33,  1.37s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run b6dc30c9-5124-49fc-9867-9789ba41bc91: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9904, Requested 315. Please try again in 1.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9904, Requested 315. Please try again in 1.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9fa579c2-3a21-48d0-a574-19cec312ba34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9872, Requested 350. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9872, Requested 350. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9837488c-6448-4b2b-9beb-6307c4eb7c6c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9904, Requested 325. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9904, Requested 325. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run df8a4fbd-88e8-42c8-9649-6c6ab8bdddc4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9866, Requested 361. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9866, Requested 361. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9a58cdb-cf98-46cc-9027-034f828b5854: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9820, Requested 409. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9820, Requested 409. Please try again in 1.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9fa579c2-3a21-48d0-a574-19cec312ba34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9948, Requested 294. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9948, Requested 294. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 008e7463-2fbe-4d4a-93d8-969d01f04c0e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9928, Requested 319. Please try again in 1.482s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9928, Requested 319. Please try again in 1.482s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4659a56c-f7c0-4e53-a6e8-aa429a5b11b5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9882, Requested 438. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9882, Requested 438. Please try again in 1.92s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:37,  2.09s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run df8a4fbd-88e8-42c8-9649-6c6ab8bdddc4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9768, Requested 279. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9768, Requested 279. Please try again in 282ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9a58cdb-cf98-46cc-9027-034f828b5854: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 345. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 345. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d26a25f2-3952-4175-81e0-a5d4f4c84a84: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9960, Requested 343. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9960, Requested 343. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "10it [00:38,  1.88s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 9fa579c2-3a21-48d0-a574-19cec312ba34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9781, Requested 289. Please try again in 420ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9781, Requested 289. Please try again in 420ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e0587baf-a8b3-46a5-8ae8-c33d45a6bddc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9742, Requested 327. Please try again in 414ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9742, Requested 327. Please try again in 414ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run df8a4fbd-88e8-42c8-9649-6c6ab8bdddc4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9912, Requested 299. Please try again in 1.266s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9912, Requested 299. Please try again in 1.266s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4659a56c-f7c0-4e53-a6e8-aa429a5b11b5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9822, Requested 386. Please try again in 1.248s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9822, Requested 386. Please try again in 1.248s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6914486e-8b22-4f7e-b592-fb0225af2540: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9870, Requested 356. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9870, Requested 356. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9a58cdb-cf98-46cc-9027-034f828b5854: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9878, Requested 347. Please try again in 1.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9878, Requested 347. Please try again in 1.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9fa579c2-3a21-48d0-a574-19cec312ba34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9946, Requested 291. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9946, Requested 291. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run df8a4fbd-88e8-42c8-9649-6c6ab8bdddc4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9960, Requested 276. Please try again in 1.416s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9960, Requested 276. Please try again in 1.416s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6914486e-8b22-4f7e-b592-fb0225af2540: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9797, Requested 278. Please try again in 450ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9797, Requested 278. Please try again in 450ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9a58cdb-cf98-46cc-9027-034f828b5854: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9861, Requested 343. Please try again in 1.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9861, Requested 343. Please try again in 1.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4659a56c-f7c0-4e53-a6e8-aa429a5b11b5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9796, Requested 376. Please try again in 1.032s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9796, Requested 376. Please try again in 1.032s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c99f9ace-93af-4138-bb2e-b7b64cd443ce: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9977, Requested 247. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9977, Requested 247. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9fa579c2-3a21-48d0-a574-19cec312ba34: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 304. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 304. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "11it [00:46,  3.70s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run df8a4fbd-88e8-42c8-9649-6c6ab8bdddc4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 315. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 315. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 008e7463-2fbe-4d4a-93d8-969d01f04c0e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9969, Requested 258. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9969, Requested 258. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9837488c-6448-4b2b-9beb-6307c4eb7c6c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9962, Requested 264. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9962, Requested 264. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6914486e-8b22-4f7e-b592-fb0225af2540: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9928, Requested 294. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9928, Requested 294. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9a58cdb-cf98-46cc-9027-034f828b5854: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9863, Requested 362. Please try again in 1.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9863, Requested 362. Please try again in 1.35s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [00:48,  2.42s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 9043d8e4-b4d1-454e-bf64-4c505505cea4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9914, Requested 307. Please try again in 1.326s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9914, Requested 307. Please try again in 1.326s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4840395e-bfd6-4900-98b9-2349990ef610: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9890, Requested 330. Please try again in 1.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9890, Requested 330. Please try again in 1.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4659a56c-f7c0-4e53-a6e8-aa429a5b11b5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9840, Requested 384. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9840, Requested 384. Please try again in 1.344s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "14it [00:50,  2.31s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6914486e-8b22-4f7e-b592-fb0225af2540: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9790, Requested 275. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9790, Requested 275. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9837488c-6448-4b2b-9beb-6307c4eb7c6c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9793, Requested 272. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9793, Requested 272. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9043d8e4-b4d1-454e-bf64-4c505505cea4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9781, Requested 255. Please try again in 216ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9781, Requested 255. Please try again in 216ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4840395e-bfd6-4900-98b9-2349990ef610: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9970, Requested 267. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9970, Requested 267. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9837488c-6448-4b2b-9beb-6307c4eb7c6c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9939, Requested 279. Please try again in 1.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9939, Requested 279. Please try again in 1.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "15it [00:53,  2.49s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6914486e-8b22-4f7e-b592-fb0225af2540: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9910, Requested 309. Please try again in 1.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9910, Requested 309. Please try again in 1.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "16it [00:53,  1.85s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 4659a56c-f7c0-4e53-a6e8-aa429a5b11b5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9826, Requested 391. Please try again in 1.302s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9826, Requested 391. Please try again in 1.302s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "17it [00:54,  1.47s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 4840395e-bfd6-4900-98b9-2349990ef610: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9973, Requested 269. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9973, Requested 269. Please try again in 1.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e0587baf-a8b3-46a5-8ae8-c33d45a6bddc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9965, Requested 262. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9965, Requested 262. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4840395e-bfd6-4900-98b9-2349990ef610: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9957, Requested 265. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9957, Requested 265. Please try again in 1.332s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "18it [01:00,  2.81s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run e0587baf-a8b3-46a5-8ae8-c33d45a6bddc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9777, Requested 281. Please try again in 348ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9777, Requested 281. Please try again in 348ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "19it [01:00,  2.02s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 4840395e-bfd6-4900-98b9-2349990ef610: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9775, Requested 284. Please try again in 354ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9775, Requested 284. Please try again in 354ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "22it [01:07,  3.06s/it]\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_data_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "unlabeled_coherence_evaluator = LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"coherence\"}, prepare_data=prepare_data_noref)\n",
        "labeled_relevance_evaluator = LangChainStringEvaluator(\"labeled_criteria\", config={ \"criteria\": \"relevance\"}, prepare_data=prepare_data_ref)\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    base_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwhBxlxYAdno"
      },
      "source": [
        "## Testing Other Retrievers\n",
        "\n",
        "Now we can test our how changing our Retriever impacts our LangSmith evaluation!\n",
        "\n",
        "We'll build this simple qa_chain factory to create standardized qa_chains where the only different component will be the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "qnfy4VNkzZi2"
      },
      "outputs": [],
      "source": [
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "  created_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | RunnablePassthrough.assign(\n",
        "        context=itemgetter(\"context\")\n",
        "      )\n",
        "    | {\n",
        "         \"response\": base_rag_prompt | primary_qa_llm,\n",
        "         \"context\": itemgetter(\"context\"),\n",
        "      }\n",
        "  )\n",
        "  return created_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPp4Xq7AvEx"
      },
      "source": [
        "### Task 1: Parent Document Retriever\n",
        "\n",
        "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
        "\n",
        "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
        "\n",
        "The basic outline of this retrieval method is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Retrieve child documents using Dense Vector Retrieval\n",
        "3. Merge the child documents based on their parents. If they have the same parents - they become merged.\n",
        "4. Replace the child documents with their respective parent documents from an in-memory-store.\n",
        "5. Use the parent documents to augment generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "67I6QJAJ0Un7"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=\"split_parents\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vectorstore = Qdrant(client, collection_name=\"split_parents\", embeddings=base_embeddings_model)\n",
        "\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "zfk5RYUt00Pw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=RecursiveCharacterTextSplitter(chunk_size=400),\n",
        "    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "68c1t4o104AK"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTH0MDolBndm"
      },
      "source": [
        "Let's create, test, and then evaluate our new chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "KMjLfqOC09Iw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Rv8bAHPN1H4P",
        "outputId": "46487575-438d-4c12-d306-7a4341d7ed83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Model) application development. It involves connecting LLMs to external data sources to retrieve relevant documents based on a user query and generate an answer grounded in the retrieved context.'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6H7vHCt2HJi"
      },
      "source": [
        "#### Evaluating the Parent Document Retrieval Pipeline\n",
        "\n",
        "Now that we've created a new retriever - let's try evaluating it on the same dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "142e5756a6c840aeb25b733ece015ed2",
            "4777a17d7d9c444a8d44e070faef0ad0",
            "dad645f3259c41b5a50e0e94906ca3e3",
            "c67428191d074e4d80cb0f28fb65cd41",
            "8db8082c5ab741b5b4fd78027e65135d",
            "2ab8452345f24b46b172515ddc77fdb0",
            "d5ad942ac5ed41c6a581b4022f177114",
            "27e2bff47a084ed6bc7d96ac09af9ebc",
            "d097ee02256d40d5b395d37cd0face05",
            "13c75ca5ee9d42eb8476a307f30b7528",
            "e8143f8ceee6468c98433734ff59b835"
          ]
        },
        "id": "Z-0WFCtx2N4n",
        "outputId": "341b1af3-cd74-46a9-d9d1-5ab2190ec96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Parent Document Retrieval RAG Evaluation-50bfef14' at:\n",
            "https://smith.langchain.com/o/42414e9e-b418-58ec-bdcf-7afcba385c60/datasets/4584aa0d-24c9-40dc-8ad9-bc95d21c61ff/compare?selectedSessions=71759b00-cf05-431e-b88c-548cd01b92e5\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 33e48efc-5de8-4049-8af7-aa0cb87a185d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9785, Requested 446. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9785, Requested 446. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 32c28861-97cc-483b-9006-411d8de140d9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9750, Requested 355. Please try again in 630ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9750, Requested 355. Please try again in 630ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8c9688c3-a0b3-4d9a-99b0-c0d1c828f361: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9707, Requested 335. Please try again in 251ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9707, Requested 335. Please try again in 251ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c352e268-9a12-47f2-9b8e-007a14780fe3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9700, Requested 340. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9700, Requested 340. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6a213bf4-44d2-40c5-b7d5-8d9e8c2933e1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9938, Requested 373. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9938, Requested 373. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 56cc4965-ed6c-4c2b-90eb-fa13eab8b2d0: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9900, Requested 406. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9900, Requested 406. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 32c28861-97cc-483b-9006-411d8de140d9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9753, Requested 373. Please try again in 756ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9753, Requested 373. Please try again in 756ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "1it [00:26, 26.11s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 33e48efc-5de8-4049-8af7-aa0cb87a185d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9716, Requested 412. Please try again in 768ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9716, Requested 412. Please try again in 768ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9cacaf0-3f44-41a5-9bb8-5f527f9c3365: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9647, Requested 481. Please try again in 768ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9647, Requested 481. Please try again in 768ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c352e268-9a12-47f2-9b8e-007a14780fe3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9963, Requested 361. Please try again in 1.944s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9963, Requested 361. Please try again in 1.944s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "2it [00:26, 11.27s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 568f22a3-9818-43b4-9885-e141e5c99bae: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9968, Requested 355. Please try again in 1.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9968, Requested 355. Please try again in 1.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6a213bf4-44d2-40c5-b7d5-8d9e8c2933e1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9907, Requested 414. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9907, Requested 414. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "3it [00:27,  6.25s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run eea072d7-138a-439f-a7f7-b9d5baff8f11: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9944, Requested 385. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9944, Requested 385. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 56cc4965-ed6c-4c2b-90eb-fa13eab8b2d0: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9918, Requested 413. Please try again in 1.985s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9918, Requested 413. Please try again in 1.985s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 33e48efc-5de8-4049-8af7-aa0cb87a185d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9872, Requested 461. Please try again in 1.998s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9872, Requested 461. Please try again in 1.998s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "4it [00:29,  4.79s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 28eb8c6d-637a-4c48-bb96-2fdb7a090b3a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9722, Requested 397. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9722, Requested 397. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 568f22a3-9818-43b4-9885-e141e5c99bae: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9728, Requested 395. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9728, Requested 395. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "5it [00:31,  3.67s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 67751187-2315-49c1-a05c-1227749473e8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9662, Requested 458. Please try again in 720ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9662, Requested 458. Please try again in 720ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9cacaf0-3f44-41a5-9bb8-5f527f9c3365: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9648, Requested 476. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9648, Requested 476. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eea072d7-138a-439f-a7f7-b9d5baff8f11: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9679, Requested 335. Please try again in 84ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9679, Requested 335. Please try again in 84ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 56cc4965-ed6c-4c2b-90eb-fa13eab8b2d0: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9929, Requested 421. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9929, Requested 421. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "6it [00:32,  2.78s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run cbecad97-4798-4c31-b07c-76fb9595b426: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9912, Requested 439. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9912, Requested 439. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "7it [00:34,  2.35s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 819774ed-5ed7-4bf3-911c-9f198037fa42: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9754, Requested 366. Please try again in 720ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9754, Requested 366. Please try again in 720ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "8it [00:34,  1.80s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 018b8040-b95c-411f-a039-630c24ec2d89: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9657, Requested 466. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9657, Requested 466. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cbecad97-4798-4c31-b07c-76fb9595b426: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9939, Requested 357. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9939, Requested 357. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 45d820ce-e9ee-4037-b6ef-315dc0ccd2fc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9925, Requested 368. Please try again in 1.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9925, Requested 368. Please try again in 1.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5fc4835f-0356-46aa-ae7e-e9d34b69acc1: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9910, Requested 385. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9910, Requested 385. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67751187-2315-49c1-a05c-1227749473e8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9893, Requested 401. Please try again in 1.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9893, Requested 401. Please try again in 1.764s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f3868fbe-d671-420f-bc9a-0c53e0a57250: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9819, Requested 487. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9819, Requested 487. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d9cacaf0-3f44-41a5-9bb8-5f527f9c3365: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9810, Requested 497. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9810, Requested 497. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:38,  2.31s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 45d820ce-e9ee-4037-b6ef-315dc0ccd2fc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9664, Requested 374. Please try again in 228ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9664, Requested 374. Please try again in 228ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "10it [00:39,  1.97s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run cbecad97-4798-4c31-b07c-76fb9595b426: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9662, Requested 378. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9662, Requested 378. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67751187-2315-49c1-a05c-1227749473e8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9970, Requested 397. Please try again in 2.202s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9970, Requested 397. Please try again in 2.202s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 018b8040-b95c-411f-a039-630c24ec2d89: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 415. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 415. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a4688a10-bc12-4cfe-9578-ef330606b591: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9928, Requested 441. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9928, Requested 441. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 66e5e663-2df3-42b6-ad4c-e9ac6c550b2c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9902, Requested 421. Please try again in 1.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9902, Requested 421. Please try again in 1.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f3868fbe-d671-420f-bc9a-0c53e0a57250: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9903, Requested 424. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9903, Requested 424. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a4688a10-bc12-4cfe-9578-ef330606b591: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9756, Requested 363. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9756, Requested 363. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 018b8040-b95c-411f-a039-630c24ec2d89: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9713, Requested 405. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9713, Requested 405. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67751187-2315-49c1-a05c-1227749473e8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9717, Requested 399. Please try again in 696ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9717, Requested 399. Please try again in 696ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8eb5ba66-97c9-4240-8b5c-91414e93431b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9702, Requested 415. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9702, Requested 415. Please try again in 702ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 28eb8c6d-637a-4c48-bb96-2fdb7a090b3a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9957, Requested 335. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9957, Requested 335. Please try again in 1.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 66e5e663-2df3-42b6-ad4c-e9ac6c550b2c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9936, Requested 359. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9936, Requested 359. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f3868fbe-d671-420f-bc9a-0c53e0a57250: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9870, Requested 426. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9870, Requested 426. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a4688a10-bc12-4cfe-9578-ef330606b591: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9931, Requested 379. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9931, Requested 379. Please try again in 1.86s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run eea072d7-138a-439f-a7f7-b9d5baff8f11: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9836, Requested 339. Please try again in 1.05s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9836, Requested 339. Please try again in 1.05s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8eb5ba66-97c9-4240-8b5c-91414e93431b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9826, Requested 352. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9826, Requested 352. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "11it [00:48,  4.20s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 66e5e663-2df3-42b6-ad4c-e9ac6c550b2c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9819, Requested 360. Please try again in 1.074s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9819, Requested 360. Please try again in 1.074s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 018b8040-b95c-411f-a039-630c24ec2d89: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9768, Requested 412. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9768, Requested 412. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 67751187-2315-49c1-a05c-1227749473e8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9766, Requested 412. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9766, Requested 412. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "12it [00:48,  3.04s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f3868fbe-d671-420f-bc9a-0c53e0a57250: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9757, Requested 421. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9757, Requested 421. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a4688a10-bc12-4cfe-9578-ef330606b591: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9943, Requested 360. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9943, Requested 360. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 66e5e663-2df3-42b6-ad4c-e9ac6c550b2c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9973, Requested 356. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9973, Requested 356. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cbecad97-4798-4c31-b07c-76fb9595b426: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9934, Requested 393. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9934, Requested 393. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [00:51,  3.03s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6795da2e-3d57-4a1d-9e24-c42578be2b6c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9931, Requested 395. Please try again in 1.955s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9931, Requested 395. Please try again in 1.955s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f32bcf9-98e4-4542-9ad4-a6c10d16f16a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 407. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 407. Please try again in 1.974s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 018b8040-b95c-411f-a039-630c24ec2d89: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9907, Requested 420. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9907, Requested 420. Please try again in 1.962s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "14it [00:52,  2.19s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run f3868fbe-d671-420f-bc9a-0c53e0a57250: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9885, Requested 441. Please try again in 1.955s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9885, Requested 441. Please try again in 1.955s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "15it [00:52,  1.59s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run a4688a10-bc12-4cfe-9578-ef330606b591: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9704, Requested 395. Please try again in 594ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9704, Requested 395. Please try again in 594ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "16it [00:54,  1.64s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6795da2e-3d57-4a1d-9e24-c42578be2b6c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9970, Requested 343. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9970, Requested 343. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 28eb8c6d-637a-4c48-bb96-2fdb7a090b3a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9960, Requested 351. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9960, Requested 351. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "17it [00:54,  1.39s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 66e5e663-2df3-42b6-ad4c-e9ac6c550b2c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9640, Requested 375. Please try again in 90ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9640, Requested 375. Please try again in 90ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "18it [00:55,  1.23s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 8eb5ba66-97c9-4240-8b5c-91414e93431b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9959, Requested 350. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9959, Requested 350. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8eb5ba66-97c9-4240-8b5c-91414e93431b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9944, Requested 369. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9944, Requested 369. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "20it [01:02,  2.15s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6f32bcf9-98e4-4542-9ad4-a6c10d16f16a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9728, Requested 346. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9728, Requested 346. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "22it [01:17,  3.52s/it]\n"
          ]
        }
      ],
      "source": [
        "pdr_rag_results = evaluate(\n",
        "    parent_document_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Parent Document Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNk6o7_BqX8"
      },
      "source": [
        "### Task 2: Ensemble Retrieval\n",
        "\n",
        "Next let's look at ensemble retrieval!\n",
        "\n",
        "You can read more about this [here](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)!\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Hit the Retriever Pair\n",
        "    - Retrieve Documents with BM25 Sparse Vector Retrieval\n",
        "    - Retrieve Documents with Dense Vector Retrieval Method\n",
        "3. Collect and \"fuse\" the retrieved docs based on their weighting using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm into a single ranked list.\n",
        "4. Use those documents to augment our generation.\n",
        "\n",
        "Ensure your `weights` list - the relative weighting of each retriever - sums to 1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "zz7dl1GD5-L-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Vs8wxT9b5pRA"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=75)\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(split_documents)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = Qdrant.from_documents(split_documents, embedding, location=\":memory:\")\n",
        "qdrant_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[\n",
        "        bm25_retriever,\n",
        "        qdrant_retriever\n",
        "    ],\n",
        "    weights=[\n",
        "        0.5,\n",
        "        0.5\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "cv69YDpF6PrJ"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6lSszzrf6UmP",
        "outputId": "ea13ffbc-df0f-4191-f873-6c2f0405d874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Models) application development. It involves connecting LLMs to external data sources to address the lack of recent or private information in the models.'"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "e99b5317081e42c199a297f3a483bbf5",
            "42bfd25ca7a14d629e4070b093a197d5",
            "fa9f9fc79b344271adc94acc487aef59",
            "a603cf6ec7d544a1a951fb8defccb976",
            "7448f6f5d9dc431fba689d8821285ca3",
            "1d0589c63fd1461b93b403c594b6ccfa",
            "5e500672dafc4529806e4e5f72fe97fa",
            "ece363bdc8b54631beba2628e3175e0b",
            "46a7a454e9bb486588ac33156abddecc",
            "f79f87ac933b45fa9e2c17871d4f2e44",
            "a888bb92387549fea8843f20e3b4c50f"
          ]
        },
        "id": "GVBY5lhm4KG7",
        "outputId": "d48d2604-1ac3-4b8a-c4aa-93214b4c0e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Hybrid Retrieval RAG Evaluation-8d359a0a' at:\n",
            "https://smith.langchain.com/o/42414e9e-b418-58ec-bdcf-7afcba385c60/datasets/4584aa0d-24c9-40dc-8ad9-bc95d21c61ff/compare?selectedSessions=ca2c7b4f-6657-4880-b299-2adb36f9ffe7\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 663561cc-1267-4ba2-81f0-b7dac83d9c92: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9736, Requested 374. Please try again in 660ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9736, Requested 374. Please try again in 660ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 282ad411-788f-4e2b-8800-538d10a2b83f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9718, Requested 391. Please try again in 654ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9718, Requested 391. Please try again in 654ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3733af81-1a29-46f0-a43c-d8e69f6f3fd4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9704, Requested 408. Please try again in 672ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9704, Requested 408. Please try again in 672ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4fa4a457-21c5-4f06-9d9e-74399f6357b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9967, Requested 340. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9967, Requested 340. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7154c92-8cd8-4131-8958-8aa47346904d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9965, Requested 342. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9965, Requested 342. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 663561cc-1267-4ba2-81f0-b7dac83d9c92: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9942, Requested 348. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9942, Requested 348. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 282ad411-788f-4e2b-8800-538d10a2b83f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9921, Requested 367. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9921, Requested 367. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3733af81-1a29-46f0-a43c-d8e69f6f3fd4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9882, Requested 403. Please try again in 1.71s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9882, Requested 403. Please try again in 1.71s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 625a4583-e344-4a57-ac95-23ba543cbe79: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9854, Requested 435. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9854, Requested 435. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4fa4a457-21c5-4f06-9d9e-74399f6357b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9942, Requested 361. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9942, Requested 361. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "1it [00:26, 26.46s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run a7154c92-8cd8-4131-8958-8aa47346904d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9940, Requested 366. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9940, Requested 366. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 165d9ff8-ce5e-44e4-92a0-3dc407020efe: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9924, Requested 381. Please try again in 1.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9924, Requested 381. Please try again in 1.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 625a4583-e344-4a57-ac95-23ba543cbe79: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9805, Requested 401. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9805, Requested 401. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 282ad411-788f-4e2b-8800-538d10a2b83f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9798, Requested 406. Please try again in 1.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9798, Requested 406. Please try again in 1.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "3it [00:28,  7.71s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 3733af81-1a29-46f0-a43c-d8e69f6f3fd4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9783, Requested 423. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9783, Requested 423. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 663561cc-1267-4ba2-81f0-b7dac83d9c92: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9817, Requested 389. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9817, Requested 389. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "5it [00:28,  3.78s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run fae4534e-e4fa-47a1-8a52-43ad02d38328: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 397. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 397. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 165d9ff8-ce5e-44e4-92a0-3dc407020efe: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9916, Requested 390. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9916, Requested 390. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "6it [00:30,  3.09s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 6cc5fa50-e8ce-4a90-a4d4-878060c71b01: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9920, Requested 387. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9920, Requested 387. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2671e1f5-f96d-4c0a-8221-210420c1287e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9728, Requested 380. Please try again in 648ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9728, Requested 380. Please try again in 648ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3d5979ec-4739-4312-bb2f-998d637cc35c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9769, Requested 342. Please try again in 666ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9769, Requested 342. Please try again in 666ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "7it [00:32,  2.79s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 52839b9c-bb9f-4349-aac4-a9e8cc4c91cd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9663, Requested 445. Please try again in 648ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9663, Requested 445. Please try again in 648ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 625a4583-e344-4a57-ac95-23ba543cbe79: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9655, Requested 451. Please try again in 636ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9655, Requested 451. Please try again in 636ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "8it [00:32,  2.12s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run fae4534e-e4fa-47a1-8a52-43ad02d38328: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9967, Requested 339. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9967, Requested 339. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 136db1e7-24cf-456e-b625-9bfbafee20d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9644, Requested 459. Please try again in 618ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9644, Requested 459. Please try again in 618ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 00255ca8-d40d-4cc1-9740-9b2e989ba45f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9865, Requested 441. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9865, Requested 441. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 81c81dd8-5b46-4df6-bdfe-6489f5c59131: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 350. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 350. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:34,  2.14s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 00255ca8-d40d-4cc1-9740-9b2e989ba45f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 385. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 385. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 136db1e7-24cf-456e-b625-9bfbafee20d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 395. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 395. Please try again in 1.836s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 52839b9c-bb9f-4349-aac4-a9e8cc4c91cd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9913, Requested 394. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9913, Requested 394. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 10c24ba7-0a17-4d41-bc78-56555031c567: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9901, Requested 407. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9901, Requested 407. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4665be2-2b48-457b-b01e-30cbf46c1c30: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9868, Requested 440. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9868, Requested 440. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "10it [00:37,  2.25s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run edb9c66b-fb29-4afb-bc0a-5628d1e9b56c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9920, Requested 393. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9920, Requested 393. Please try again in 1.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fae4534e-e4fa-47a1-8a52-43ad02d38328: ValueError(\"Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth or reference answer is not provided. The assistant's response indicates that the video_manifest for the generated video will include 12 scenes, but without a reference answer, it's impossible to determine if this is correct. Rating: [[N/A]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
            "    return self.create_outputs(response)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 285, in create_outputs\n",
            "    self.output_key: self.output_parser.parse_result(generation),\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py\", line 221, in parse_result\n",
            "    return self.parse(result[0].text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 134, in parse\n",
            "    raise ValueError(\n",
            "ValueError: Invalid output: The assistant's response cannot be evaluated for accuracy as the ground truth or reference answer is not provided. The assistant's response indicates that the video_manifest for the generated video will include 12 scenes, but without a reference answer, it's impossible to determine if this is correct. Rating: [[N/A]]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4665be2-2b48-457b-b01e-30cbf46c1c30: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9810, Requested 358. Please try again in 1.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9810, Requested 358. Please try again in 1.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 52839b9c-bb9f-4349-aac4-a9e8cc4c91cd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9786, Requested 383. Please try again in 1.013s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9786, Requested 383. Please try again in 1.013s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 00255ca8-d40d-4cc1-9740-9b2e989ba45f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9786, Requested 380. Please try again in 996ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9786, Requested 380. Please try again in 996ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 136db1e7-24cf-456e-b625-9bfbafee20d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9764, Requested 397. Please try again in 966ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9764, Requested 397. Please try again in 966ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 482a4d4e-6135-44a5-80ea-5229db9ee881: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9744, Requested 420. Please try again in 984ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9744, Requested 420. Please try again in 984ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run edb9c66b-fb29-4afb-bc0a-5628d1e9b56c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9949, Requested 341. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9949, Requested 341. Please try again in 1.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6cc5fa50-e8ce-4a90-a4d4-878060c71b01: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 325. Please try again in 1.674s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9954, Requested 325. Please try again in 1.674s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fae4534e-e4fa-47a1-8a52-43ad02d38328: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9969, Requested 336. Please try again in 1.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9969, Requested 336. Please try again in 1.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4665be2-2b48-457b-b01e-30cbf46c1c30: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9924, Requested 379. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9924, Requested 379. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 00255ca8-d40d-4cc1-9740-9b2e989ba45f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 382. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9922, Requested 382. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 52839b9c-bb9f-4349-aac4-a9e8cc4c91cd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9912, Requested 391. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9912, Requested 391. Please try again in 1.818s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 136db1e7-24cf-456e-b625-9bfbafee20d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 393. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9911, Requested 393. Please try again in 1.824s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2671e1f5-f96d-4c0a-8221-210420c1287e: ValueError(\"Invalid output: The assistant's response cannot be evaluated for accuracy as the piece of context or reference answer is not provided. Without these, it's impossible to determine if the technologies mentioned (SQL, Pandas, GPTs, and the Assistant API) are accurate or not. Therefore, I cannot provide a rating.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
            "    return self.create_outputs(response)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 285, in create_outputs\n",
            "    self.output_key: self.output_parser.parse_result(generation),\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py\", line 221, in parse_result\n",
            "    return self.parse(result[0].text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 134, in parse\n",
            "    raise ValueError(\n",
            "ValueError: Invalid output: The assistant's response cannot be evaluated for accuracy as the piece of context or reference answer is not provided. Without these, it's impossible to determine if the technologies mentioned (SQL, Pandas, GPTs, and the Assistant API) are accurate or not. Therefore, I cannot provide a rating.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6cc5fa50-e8ce-4a90-a4d4-878060c71b01: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9844, Requested 334. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9844, Requested 334. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 482a4d4e-6135-44a5-80ea-5229db9ee881: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9822, Requested 358. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9822, Requested 358. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run fae4534e-e4fa-47a1-8a52-43ad02d38328: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9673, Requested 351. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9673, Requested 351. Please try again in 144ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "11it [00:47,  4.47s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 2671e1f5-f96d-4c0a-8221-210420c1287e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9982, Requested 326. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9982, Requested 326. Please try again in 1.848s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6cc5fa50-e8ce-4a90-a4d4-878060c71b01: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9968, Requested 341. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9968, Requested 341. Please try again in 1.854s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "12it [00:47,  3.22s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 10c24ba7-0a17-4d41-bc78-56555031c567: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9965, Requested 346. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9965, Requested 346. Please try again in 1.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4665be2-2b48-457b-b01e-30cbf46c1c30: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9957, Requested 355. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9957, Requested 355. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 52839b9c-bb9f-4349-aac4-a9e8cc4c91cd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9913, Requested 399. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9913, Requested 399. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [00:47,  2.37s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 00255ca8-d40d-4cc1-9740-9b2e989ba45f: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9868, Requested 395. Please try again in 1.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9868, Requested 395. Please try again in 1.578s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "14it [00:47,  1.75s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 136db1e7-24cf-456e-b625-9bfbafee20d7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9893, Requested 412. Please try again in 1.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9893, Requested 412. Please try again in 1.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "15it [00:49,  1.76s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 482a4d4e-6135-44a5-80ea-5229db9ee881: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9885, Requested 359. Please try again in 1.464s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 352, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9885, Requested 359. Please try again in 1.464s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 10c24ba7-0a17-4d41-bc78-56555031c567: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9720, Requested 338. Please try again in 348ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9720, Requested 338. Please try again in 348ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e4665be2-2b48-457b-b01e-30cbf46c1c30: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9665, Requested 394. Please try again in 354ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9665, Requested 394. Please try again in 354ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "16it [00:51,  1.79s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 7155fa06-c476-4f0e-a316-6bdeef47ce91: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9646, Requested 412. Please try again in 348ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9646, Requested 412. Please try again in 348ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 519005b8-c75c-40c3-8094-4804d30a2c63: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9967, Requested 438. Please try again in 2.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 307, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9967, Requested 438. Please try again in 2.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 482a4d4e-6135-44a5-80ea-5229db9ee881: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9964, Requested 355. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9964, Requested 355. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "17it [00:53,  1.82s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 10c24ba7-0a17-4d41-bc78-56555031c567: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9961, Requested 361. Please try again in 1.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9961, Requested 361. Please try again in 1.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "18it [00:54,  1.61s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 519005b8-c75c-40c3-8094-4804d30a2c63: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9961, Requested 360. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9961, Requested 360. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 482a4d4e-6135-44a5-80ea-5229db9ee881: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9947, Requested 374. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1233, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 568, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 565, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 257, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
            "    raise e\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 549, in _generate\n",
            "    response = self.client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1015, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\user\\Projects\\aie3\\public-aie3-solutions\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-YnU9T1kh90y03X5883kzXfON on tokens per min (TPM): Limit 10000, Used 9947, Requested 374. Please try again in 1.925s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "22it [01:18,  3.55s/it]\n"
          ]
        }
      ],
      "source": [
        "pdr_rag_results = evaluate(\n",
        "    ensemble_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Hybrid Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### â“Question #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "Describe in your own words what the metrics are expressing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "087e4c759a35424385156c76e3780144": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef2e833ac73548ee837f906d4b004a3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2c984bd9861c4ec0b27f0c3a2020dede",
            "value": ""
          }
        },
        "13c75ca5ee9d42eb8476a307f30b7528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142e5756a6c840aeb25b733ece015ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4777a17d7d9c444a8d44e070faef0ad0",
              "IPY_MODEL_dad645f3259c41b5a50e0e94906ca3e3",
              "IPY_MODEL_c67428191d074e4d80cb0f28fb65cd41"
            ],
            "layout": "IPY_MODEL_8db8082c5ab741b5b4fd78027e65135d"
          }
        },
        "18ffc7ae91d1408c8747f8d0a972f3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9927b2052be4016b0feb88ab583d32e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fbddf6663954ff48ee0cb0b4a202acb",
            "value": 1
          }
        },
        "1d0589c63fd1461b93b403c594b6ccfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e2bff47a084ed6bc7d96ac09af9ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2ab8452345f24b46b172515ddc77fdb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c984bd9861c4ec0b27f0c3a2020dede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bfd25ca7a14d629e4070b093a197d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0589c63fd1461b93b403c594b6ccfa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e500672dafc4529806e4e5f72fe97fa",
            "value": ""
          }
        },
        "46a7a454e9bb486588ac33156abddecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4777a17d7d9c444a8d44e070faef0ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab8452345f24b46b172515ddc77fdb0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d5ad942ac5ed41c6a581b4022f177114",
            "value": ""
          }
        },
        "5e500672dafc4529806e4e5f72fe97fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65d1b263094a4c6abdaf8688285c41cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_087e4c759a35424385156c76e3780144",
              "IPY_MODEL_18ffc7ae91d1408c8747f8d0a972f3a8",
              "IPY_MODEL_8cbac09f7ba540bd8f79dde47e49fe5f"
            ],
            "layout": "IPY_MODEL_b5e2be01c1834c78b7ce0cbe1f7a9d09"
          }
        },
        "6fbddf6663954ff48ee0cb0b4a202acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7448f6f5d9dc431fba689d8821285ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbac09f7ba540bd8f79dde47e49fe5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6035d68dd9e4c6598ac5eae2d0deac1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b6b1f2be05a34e118dc55869149b60ac",
            "value": "â€‡12/?â€‡[01:07&lt;00:00,â€‡â€‡2.29s/it]"
          }
        },
        "8db8082c5ab741b5b4fd78027e65135d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a603cf6ec7d544a1a951fb8defccb976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79f87ac933b45fa9e2c17871d4f2e44",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a888bb92387549fea8843f20e3b4c50f",
            "value": "â€‡22/?â€‡[01:19&lt;00:00,â€‡â€‡2.61s/it]"
          }
        },
        "a888bb92387549fea8843f20e3b4c50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5e2be01c1834c78b7ce0cbe1f7a9d09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b1f2be05a34e118dc55869149b60ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c67428191d074e4d80cb0f28fb65cd41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c75ca5ee9d42eb8476a307f30b7528",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e8143f8ceee6468c98433734ff59b835",
            "value": "â€‡22/?â€‡[01:16&lt;00:00,â€‡â€‡2.24s/it]"
          }
        },
        "c9927b2052be4016b0feb88ab583d32e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d097ee02256d40d5b395d37cd0face05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5ad942ac5ed41c6a581b4022f177114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dad645f3259c41b5a50e0e94906ca3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e2bff47a084ed6bc7d96ac09af9ebc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d097ee02256d40d5b395d37cd0face05",
            "value": 1
          }
        },
        "e6035d68dd9e4c6598ac5eae2d0deac1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8143f8ceee6468c98433734ff59b835": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e99b5317081e42c199a297f3a483bbf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42bfd25ca7a14d629e4070b093a197d5",
              "IPY_MODEL_fa9f9fc79b344271adc94acc487aef59",
              "IPY_MODEL_a603cf6ec7d544a1a951fb8defccb976"
            ],
            "layout": "IPY_MODEL_7448f6f5d9dc431fba689d8821285ca3"
          }
        },
        "ece363bdc8b54631beba2628e3175e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ef2e833ac73548ee837f906d4b004a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79f87ac933b45fa9e2c17871d4f2e44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9f9fc79b344271adc94acc487aef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece363bdc8b54631beba2628e3175e0b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46a7a454e9bb486588ac33156abddecc",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
